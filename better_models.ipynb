{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import seed\n",
    "import collections\n",
    "import imblearn\n",
    "\n",
    "# Machine learning models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as imblearnPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "# Model evaluation and hyperparameter tuning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "\n",
    "# Constant features\n",
    "from fast_ml.utilities import display_all\n",
    "from fast_ml.feature_selection import get_constant_features\n",
    "\n",
    "# Data visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Feature selection methods\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(21)\n",
    "path_save = './Output/'\n",
    "df = pd.read_csv(path_save + 'finaldataset_for_ML.csv', encoding='latin-1')\n",
    "df2 = pd.read_csv(path_save + 'finaldataset_for_ML2.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Target \"0\": projetos encerrados / terminados\n",
    "Target \"1: projetos anulados\n",
    "\"\"\"\n",
    "def create_target_column(row):\n",
    "    if(row['Terminado'] == 1.0):\n",
    "        return 0\n",
    "    elif(row['Anulado'] == 1.0):\n",
    "        return 1\n",
    "    else:\n",
    "        print(\"Existe um registo sem target\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Target'] = df.apply(lambda x: create_target_column(x), axis=1)\n",
    "df2['Target'] = df2.apply(lambda x: create_target_column(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df.replace(np.nan, 0.0) #Replace nulls for 0\n",
    "clean_df.replace([np.inf, -np.inf], 0.0, inplace=True) #Replace infinites for 0\n",
    "\n",
    "clean_df2 = df2.replace(np.nan, 0.0) #Replace nulls for 0\n",
    "clean_df2.replace([np.inf, -np.inf], 0.0, inplace=True) #Replace infinites for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     resultado_das_atividades_descontinuadas\n",
       "1                          IAE_CMVMC_ACT_BIOL\n",
       "2                                        rank\n",
       "3                        Uploads/Aplicavel_12\n",
       "4                      3_IMG_EC_RS_SERSOCIAIS\n",
       "                       ...                   \n",
       "66                     Incentivo/Cap_Proprios\n",
       "67                      Incentivo/Dispensa_Ic\n",
       "68                          Paramproj/Param_1\n",
       "69                             Resumo/Icep_75\n",
       "70                         Impactoemp/Impacto\n",
       "Name: Var, Length: 71, dtype: object"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_features = get_constant_features(clean_df)\n",
    "#constant_features\n",
    "\n",
    "constant_features['Var'] #Shows features that have constant values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existem 82 colunas constantes\n"
     ]
    }
   ],
   "source": [
    "constant_features_list = constant_features['Var'].tolist()\n",
    "constant_features_list.append('N_Proj_anon')\n",
    "constant_features_list.append('CAE_SUBCLASSE')\n",
    "constant_features_list.append('DATA_RECEPCAO')\n",
    "constant_features_list.append('NIF_anon')\n",
    "constant_features_list.append('Terminado')\n",
    "constant_features_list.append('Anulado')\n",
    "constant_features_list.append('Nproj_anon_x')\n",
    "constant_features_list.append('Nproj_anon_y')\n",
    "constant_features_list.append('ANO_EXERCICIO')\n",
    "constant_features_list.append('ANO_EXERCICIO_VALIDOS')\n",
    "constant_features_list.append('Parametros/Ano_Cand')\n",
    "print('Existem %i colunas constantes' % len(constant_features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(constant_features_list, axis=1)\n",
    "clean_df2 = clean_df2.drop(constant_features_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = clean_df.select_dtypes(include='object').columns\n",
    "object_columns_list = object_columns.tolist()\n",
    "object_columns_list.remove(\"Motivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = clean_df.drop(columns=object_columns_list)\n",
    "clean_df2 = clean_df2.drop(columns=object_columns_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "projetos_encerrados = clean_df.loc[clean_df['Target'] == 0]\n",
    "projetos_anulados_motivos2 = clean_df.loc[(clean_df['Target'] == 1) & (clean_df['Motivo'] != \"DesistÃªncia do promotor\")]\n",
    "clean_df4 = pd.concat([projetos_encerrados, projetos_anulados_motivos2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = clean_df.drop(columns=['Target', 'Motivo'])\n",
    "y = clean_df['Target']\n",
    "scaler = MinMaxScaler() #MinMaxScaler\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X.values), columns= X.columns, index=X.index)\n",
    "X3 = clean_df2.drop(columns=['Target', 'Motivo'])\n",
    "y3 = clean_df2['Target']\n",
    "X4 = clean_df4.drop(columns=['Target', 'Motivo'])\n",
    "y4 = clean_df4['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=21, train_size=0.7, test_size=0.3)\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, random_state=21, train_size=0.7, test_size=0.3)\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X4, y4, random_state=21, train_size=0.7, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experience</th>\n",
       "      <th>Count</th>\n",
       "      <th>Columns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>feat1</td>\n",
       "      <td>30</td>\n",
       "      <td>RES_ANTES_DEPRECIACAO_GASTOS, GASTOS_DEPRECIAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>feat2</td>\n",
       "      <td>30</td>\n",
       "      <td>GASTOS_PESSOAL, OUTROS_REDIMENTOS_GANHOS, RES_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feat3</td>\n",
       "      <td>50</td>\n",
       "      <td>GASTOS_PESSOAL, RES_ANTES_DEPRECIACAO_GASTOS, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>feat4</td>\n",
       "      <td>50</td>\n",
       "      <td>VENDAS_SERVICOS_PRESTADOS, GASTOS_PESSOAL, OUT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feat5</td>\n",
       "      <td>50</td>\n",
       "      <td>GASTOS_DEPRECIACAO_AMORTIZA, Incentivo/Tx_Limi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>feat6</td>\n",
       "      <td>50</td>\n",
       "      <td>RES_ANTES_DEPRECIACAO_GASTOS, GASTOS_DEPRECIAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feat7</td>\n",
       "      <td>50</td>\n",
       "      <td>Txtfinanc/Fonte, Promotor/Nat_Jur, 3_IMG_EC_PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>allFeat</td>\n",
       "      <td>267</td>\n",
       "      <td>VENDAS_SERVICOS_PRESTADOS, SUBSIDIOS_EXPLORACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>manualFeat</td>\n",
       "      <td>22</td>\n",
       "      <td>EBITDA, EBIT, Total_Assets, Total_Liabilities,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experience  Count                                            Columns\n",
       "0       feat1     30  RES_ANTES_DEPRECIACAO_GASTOS, GASTOS_DEPRECIAC...\n",
       "1       feat2     30  GASTOS_PESSOAL, OUTROS_REDIMENTOS_GANHOS, RES_...\n",
       "2       feat3     50  GASTOS_PESSOAL, RES_ANTES_DEPRECIACAO_GASTOS, ...\n",
       "3       feat4     50  VENDAS_SERVICOS_PRESTADOS, GASTOS_PESSOAL, OUT...\n",
       "4       feat5     50  GASTOS_DEPRECIACAO_AMORTIZA, Incentivo/Tx_Limi...\n",
       "5       feat6     50  RES_ANTES_DEPRECIACAO_GASTOS, GASTOS_DEPRECIAC...\n",
       "6       feat7     50  Txtfinanc/Fonte, Promotor/Nat_Jur, 3_IMG_EC_PR...\n",
       "7     allFeat    267  VENDAS_SERVICOS_PRESTADOS, SUBSIDIOS_EXPLORACA...\n",
       "8  manualFeat     22  EBITDA, EBIT, Total_Assets, Total_Liabilities,..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_features_list = pd.read_csv(path_save + \"table_features_experience_oficial.csv\", encoding=\"latin-1\")\n",
    "display(df_features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_features_list.loc[df_features_list['Experience'] == \"manualFeat\", [\"Columns\"]].values.tolist()[0][0].split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat5 = df_features_list.loc[df_features_list['Experience'] == \"feat5\", [\"Columns\"]].values.tolist()[0][0].split(\",\")\n",
    "feat5 = [s.strip() for s in feat5]\n",
    "feat7 = df_features_list.loc[df_features_list['Experience'] == \"feat7\", [\"Columns\"]].values.tolist()[0][0].split(\",\")\n",
    "feat7 = [s.strip() for s in feat7]\n",
    "manualFeat = df_features_list.loc[df_features_list['Experience'] == \"manualFeat\", [\"Columns\"]].values.tolist()[0][0].split(\",\")\n",
    "manualFeat = [s.strip() for s in manualFeat]\n",
    "feat5_new = feat5 + manualFeat\n",
    "feat7_new = feat7 + manualFeat\n",
    "feat5_new = list(set(feat5))\n",
    "feat7_new = list(set(feat7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GASTOS_DEPRECIACAO_AMORTIZA',\n",
       " 'Incentivo/Tx_Limite',\n",
       " 'Operating cash flow current liabilities',\n",
       " 'Dadosprojecto/N_Meses',\n",
       " 'SUBSIDIOS_EXPLORACAO',\n",
       " 'ATIVO_COR_ESTADO_OUT_ENTES_PUB',\n",
       " 'PASSIVO_COR_OUT_CONTAS_A_PAGAR',\n",
       " 'GP_REMUN_ORGAOS_SOCIAIS',\n",
       " 'IAE_VAR_INVENT_PROD',\n",
       " 'ATIVO_COR_DIFERIMENTOS',\n",
       " 'Resumo/Cae',\n",
       " 'IAE_VENDAS_MERCADORIAS',\n",
       " '2_IMG_COM_VENDAS',\n",
       " 'total debt / total assets',\n",
       " 'PASSIVO_NC_FINANCIAMENTOS_OBTD',\n",
       " 'ATIVO_NCOR_INV_FINANC_PQ_ENTID',\n",
       " 'Resumo/Nute_Norte',\n",
       " '3_IMG_EC_COMPRAS',\n",
       " 'Growth_Rate_Net_Sales_T3',\n",
       " 'NIF_Prom_anon',\n",
       " 'GP_SEG_ACID_TRAB_DOEN_PROF',\n",
       " 'IAE_AFT_QUANT_ESCR_LIQ_FIN',\n",
       " 'earnings before tax and interest / total asset',\n",
       " 'ATIVO_NCOR_PART_FINAN_EQV_PAT',\n",
       " 'CP_OUTRAS_VARIACAOES_CAP_PRO',\n",
       " 'Resumo/Investimento',\n",
       " 'IAE_AFT_TOTAL_AQUIS_EDIF',\n",
       " 'CP_RESERVAS_LEGAIS',\n",
       " 'Growth_Rate_Net_Sales_T2',\n",
       " '1_IMG_INT_FORN_SEREXTERN',\n",
       " 'PASSIVO_COR_ESTADO_OUT_ENT_PUB',\n",
       " 'IAE_PREST_SERV',\n",
       " 'IMPOSTO_RENDIMENTO_PERIODO',\n",
       " 'ATIVO_COR_ACCIONISTAS_SOCIOS',\n",
       " 'Inventory_Turnover',\n",
       " 'Total_Assets',\n",
       " 'ATIVO_COR_OUTRA_CONT_A_RECEBER',\n",
       " 'Gross income divided by sales',\n",
       " 'PASSIVO_COR_FORNCEDORES',\n",
       " 'Resumo/Distrito',\n",
       " 'ATIVO_COR_INVENTARIOS',\n",
       " 'RES_ANTES_DEPRECIACAO_GASTOS',\n",
       " 'CP_E_PASSIVO_TOTAL',\n",
       " '2_PESSOAL_NHT_PSE_MULHERES',\n",
       " 'total_assets_to_total_liabilities',\n",
       " 'PASSIVO_COR_OUTROS_PAS_CORRENTES',\n",
       " 'IAE_CMVMC_MERCADORIAS',\n",
       " 'IAE_COMPRAS',\n",
       " 'Resumo/Elegivel',\n",
       " '3_IMG_EC_REND_SUPLEM']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Txtfinanc/Fonte',\n",
       " 'Promotor/Nat_Jur',\n",
       " '3_IMG_EC_PREST_SERV',\n",
       " 'GASTOS_DEPRECIACAO_AMORTIZA',\n",
       " 'RES_ANTES_DEPRECIACAO_GASTOS',\n",
       " 'ATIVO_NCOR_PART_FINAN_EQV_PAT',\n",
       " 'Growth_Rate_Net_Sales_T3',\n",
       " 'ATIVO_NCOR_FIXOS_TANGIVEIS',\n",
       " 'ATIVO_NCOR_INV_FINANC_PQ_ENTID',\n",
       " 'Resumo/Lst_Po',\n",
       " '1_IMG_INT_AQUIS_ACT_INTANG',\n",
       " 'PROVISOES',\n",
       " 'CP_AJUST_EM_ACT_FINANCEIROS',\n",
       " 'PASSIVO_NC_FINANCIAMENTOS_OBTD',\n",
       " 'ATIVO_COR_INVENTARIOS',\n",
       " 'ATIVO_COR_ACCIONISTAS_SOCIOS',\n",
       " '3_IMG_EC_AQUIS_ACT_FIX_TANG',\n",
       " '2_N_PESSOAL_NHT_PSETP_REMUNERADAS',\n",
       " 'Resumo/Nute_Norte',\n",
       " '2_IMG_COM_AQUIS_ACT_FIX_TANG',\n",
       " '2_PESSOAL_NHT_PSE_TEMPO_PARCIAL',\n",
       " 'Critselb1/N_Mercados',\n",
       " 'Growth_Rate_Total_Assets_T3',\n",
       " 'CP_TOTAL',\n",
       " 'GP_REMUN_ORGAOS_SOCIAIS',\n",
       " '3_IMG_EC_COMPRAS',\n",
       " 'Growth_Rate_Net_Sales_T2',\n",
       " 'Growth_Rate_Net_Sales_T1',\n",
       " 'Operating cash flow current liabilities',\n",
       " 'IAE_CMVMC_MATER_PRIMAS',\n",
       " 'Resumo/Investimento',\n",
       " '1_PESSOAL_NMP_PSE_MULHERES',\n",
       " 'ATIVO_NCOR_PROPRI_INVESTIMENTO',\n",
       " 'ATIVO_COR_CLIENTES',\n",
       " 'Analisemercados/Direcao',\n",
       " '2_IMG_COM_FORN_SEREXTERN',\n",
       " 'Growth_Rate_Total_Assets_T2',\n",
       " 'ATIVO_COR_ADIANTAMENTOS_FORNEC',\n",
       " 'PASSIVO_COR_ADIANTA_DE_CLIENTES',\n",
       " 'IAE_AFT_TOTAL_AQUIS_EDIF',\n",
       " 'total_assets_to_total_liabilities',\n",
       " 'ATIVO_COR_ESTADO_OUT_ENTES_PUB',\n",
       " 'CMVMC',\n",
       " '1_IMG_INT_RS_OUTROS',\n",
       " 'CP_OUTRAS_RESERVAS',\n",
       " '1_PESSOAL_NMP_PSE_HOMENS',\n",
       " 'EBIT',\n",
       " 'IAE_AFT_TOTAL_AQUIS',\n",
       " 'Promotor/Concelho',\n",
       " 'CP_OUTRAS_VARIACAOES_CAP_PRO']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EBITDA',\n",
       " 'EBIT',\n",
       " 'Total_Assets',\n",
       " 'Total_Liabilities',\n",
       " 'total_assets_to_total_liabilities',\n",
       " 'Working capital divided by total assets',\n",
       " 'Gross income divided by sales',\n",
       " 'total debt / total assets',\n",
       " 'earnings before tax and interest / total asset',\n",
       " 'Operating cash flow current liabilities',\n",
       " 'Accounts_Receivables_Turnover',\n",
       " 'Creditors_Turnover',\n",
       " 'Inventory_Turnover',\n",
       " 'Average_Collection_Period_For_Receivables',\n",
       " 'Average_Payment_Period_To_Creditors',\n",
       " 'Average_Turnover_Period_For_Inventories',\n",
       " 'Growth_Rate_Net_Sales_T1',\n",
       " 'Growth_Rate_Net_Sales_T2',\n",
       " 'Growth_Rate_Net_Sales_T3',\n",
       " 'Growth_Rate_Total_Assets_T1',\n",
       " 'Growth_Rate_Total_Assets_T2',\n",
       " 'Growth_Rate_Total_Assets_T3']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manualFeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenario 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN ####\n",
      "Accuracy: 0.79\n",
      "F1: 0.79\n",
      "Precision: 0.77\n",
      "Recall: 0.82\n",
      "ROC_AUC: 0.79\n",
      "#### TEST ####\n",
      "Accuracy: 0.72\n",
      "F1: 0.66\n",
      "Precision: 0.63\n",
      "Recall: 0.70\n",
      "ROC_AUC: 0.72\n"
     ]
    }
   ],
   "source": [
    "pipe = imblearnPipeline([('StandardScaler', StandardScaler()), ('over', SMOTE(random_state=21)), (\"SupportVectorClassification\", SVC(random_state=21))])\n",
    "X_train_balanced, y_train_balanced = pipe['over'].fit_resample(X_train[feat5_new], y_train)\n",
    "pipe.fit(X_train_balanced, y_train_balanced)\n",
    "# Predict train dataset\n",
    "print(\"#### TRAIN ####\")\n",
    "y_pred = pipe.predict(X_train_balanced)\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_train_balanced, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_train_balanced, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_train_balanced, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_train_balanced, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_train_balanced, y_pred)))\n",
    "\n",
    "# Predict test dataset\n",
    "print(\"#### TEST ####\")\n",
    "y_pred = pipe.predict(X_test[feat5_new])\n",
    "print(\"Accuracy: %.2f\" %(accuracy_score(y_test, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_test, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_test, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN ####\n",
      "Accuracy: 0.79\n",
      "F1: 0.79\n",
      "Precision: 0.77\n",
      "Recall: 0.82\n",
      "ROC_AUC: 0.79\n",
      "#### TEST ####\n",
      "Accuracy: 0.72\n",
      "F1: 0.66\n",
      "Precision: 0.63\n",
      "Recall: 0.70\n",
      "ROC_AUC: 0.72\n"
     ]
    }
   ],
   "source": [
    "pipe = imblearnPipeline([('StandardScaler', StandardScaler()), ('under', RandomUnderSampler(random_state=21)), ('over', SMOTE(random_state=21)), (\"SupportVectorClassification\", SVC(random_state=21))])\n",
    "X_train_balanced, y_train_balanced = pipe['over'].fit_resample(X_train[feat5_new], y_train)\n",
    "pipe.fit(X_train_balanced, y_train_balanced)\n",
    "# Predict train dataset\n",
    "print(\"#### TRAIN ####\")\n",
    "y_pred = pipe.predict(X_train_balanced)\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_train_balanced, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_train_balanced, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_train_balanced, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_train_balanced, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_train_balanced, y_pred)))\n",
    "\n",
    "# Predict test dataset\n",
    "print(\"#### TEST ####\")\n",
    "y_pred = pipe.predict(X_test[feat5_new])\n",
    "print(\"Accuracy: %.2f\" %(accuracy_score(y_test, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_test, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_test, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'f1_score': make_scorer(f1_score),\n",
    "    'precision': make_scorer(precision_score),\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "pipeline = imblearnPipeline([('StandardScaler', StandardScaler()), ('resampler', SMOTE(random_state=21)), (\"classifier\", SVC(random_state=21))])\n",
    "\n",
    "# Define hyperparameters for resampler and classifier\n",
    "param_grid = {\n",
    "    'resampler__sampling_strategy': ['auto', 0.5, 0.75],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create custom scorers for F1 score and accuracy\n",
    "#f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "#accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Grid Search with multiple scoring metrics\n",
    "#scoring = {'F1': f1_scorer, 'Accuracy': accuracy_scorer}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search.fit(X_train[feat5], y_train)\n",
    "\n",
    "# Get the best estimator and best hyperparameters\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Access the results including all metrics\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Print the metrics for each combination of hyperparameters\n",
    "for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:\n",
    "    mean_metric_values = np.nanmean(results[f'mean_test_{metric}']) #results[f'mean_test_{metric}']\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_metric_values.mean():.4f}\")\n",
    "\n",
    "y_pred = best_estimator.predict(X_test[feat5])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "roc_auc = roc_auc_score(y_test, y_pred)  # ROC AUC is for binary classification\n",
    "print(\"Best estimator metrics\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"ROC AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__C': 10, 'classifier__kernel': 'rbf', 'resampler__sampling_strategy': 'auto', 'undersampler__sampling_strategy': 'auto'}\n",
      "Mean Accuracy: 0.6414\n",
      "Mean F1_score: 0.5542\n",
      "Mean Precision: 0.4964\n",
      "Mean Recall: 0.6356\n",
      "Mean Roc_auc: 0.6402\n",
      "Best estimator metrics\n",
      "Accuracy: 0.6722222222222223\n",
      "F1 Score: 0.6760398824909428\n",
      "Precision: 0.6998944658944659\n",
      "Recall: 0.6722222222222223\n",
      "ROC AUC: 0.6838628193918197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "210 fits failed out of a total of 270.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 550, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 409, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "120 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 550, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 389, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.62729541        nan 0.6213359         nan        nan        nan\n",
      "        nan        nan        nan 0.66072141        nan 0.6296122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63803821        nan 0.63685486        nan        nan        nan\n",
      "        nan        nan        nan 0.65479042        nan 0.66075706\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62727402        nan 0.6332763         nan        nan        nan\n",
      "        nan        nan        nan 0.65352866        nan 0.65355004\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.55335144        nan 0.55521165        nan        nan        nan\n",
      "        nan        nan        nan 0.51554152        nan 0.53301622\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56363184        nan 0.56767607        nan        nan        nan\n",
      "        nan        nan        nan 0.56706379        nan 0.55297873\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.54894847        nan 0.56265478        nan        nan        nan\n",
      "        nan        nan        nan 0.5809281         nan 0.54955924\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.48042366        nan 0.47699458        nan        nan        nan\n",
      "        nan        nan        nan 0.52680979        nan 0.48020458\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.49196154        nan 0.4919798         nan        nan        nan\n",
      "        nan        nan        nan 0.51106631        nan 0.51611436\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.47948254        nan 0.48800477        nan        nan        nan\n",
      "        nan        nan        nan 0.50690163        nan 0.50732152\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.65548023        nan 0.67248588        nan        nan        nan\n",
      "        nan        nan        nan 0.51694915        nan 0.60152542\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66231638        nan 0.67581921        nan        nan        nan\n",
      "        nan        nan        nan 0.6420339         nan 0.60169492\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64536723        nan 0.66903955        nan        nan        nan\n",
      "        nan        nan        nan 0.68248588        nan 0.60163842\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.63373739        nan 0.63304892        nan        nan        nan\n",
      "        nan        nan        nan 0.62820784        nan 0.62333322\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64359448        nan 0.64578422        nan        nan        nan\n",
      "        nan        nan        nan 0.65195477        nan 0.64749204\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63143319        nan 0.64145997        nan        nan        nan\n",
      "        nan        nan        nan 0.66011824        nan 0.64189124\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "pipeline = imblearnPipeline([('StandardScaler', StandardScaler()), ('undersampler', RandomUnderSampler(random_state=21)), ('resampler', SMOTE(random_state=21)), (\"classifier\", SVC(random_state=21))])\n",
    "\n",
    "# Define hyperparameters for resampler and classifier\n",
    "param_grid = {\n",
    "    'resampler__sampling_strategy': ['auto', 0.5, 0.75],\n",
    "    'undersampler__sampling_strategy': ['auto', 0.5, 0.75],\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create custom scorers for F1 score and accuracy\n",
    "#f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "#accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Grid Search with multiple scoring metrics\n",
    "#scoring = {'F1': f1_scorer, 'Accuracy': accuracy_scorer}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search.fit(X_train[feat5], y_train)\n",
    "\n",
    "# Get the best estimator and best hyperparameters\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Access the results including all metrics\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Print the metrics for each combination of hyperparameters\n",
    "for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:\n",
    "    mean_metric_values = np.nanmean(results[f'mean_test_{metric}']) #results[f'mean_test_{metric}']\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_metric_values.mean():.4f}\")\n",
    "\n",
    "y_pred = best_estimator.predict(X_test[feat5])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "roc_auc = roc_auc_score(y_test, y_pred)  # ROC AUC is for binary classification\n",
    "print(\"Best estimator metrics\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"ROC AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenario 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('StandardScaler', StandardScaler()), ('pca', PCA()), (\"MLP\", MLPClassifier(random_state=21))])\n",
    "#X_train_balanced, y_train_balanced = pipe['over'].fit_resample(X_train[feat5_new], y_train)\n",
    "pipe.fit(X_train4[feat5_new], y_train4)\n",
    "# Predict train dataset\n",
    "print(\"#### TRAIN ####\")\n",
    "y_pred = pipe.predict(y_train4)\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_train4, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_train4, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_train4, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_train4, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_train4, y_pred)))\n",
    "\n",
    "# Predict test dataset\n",
    "print(\"#### TEST ####\")\n",
    "y_pred = pipe.predict(X_test4[feat5_new])\n",
    "print(\"Accuracy: %.2f\" %(accuracy_score(y_test4, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_test4, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_test4, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_test4, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_test4, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'activation' for estimator Pipeline(steps=[('StandardScaler', StandardScaler()),\n                ('undersampler', RandomUnderSampler(random_state=21)),\n                ('resampler', SMOTE(random_state=21)),\n                ('classifier', RandomForestClassifier(random_state=21))]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 27\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Create custom scorers for F1 score and accuracy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#f1_scorer = make_scorer(f1_score, average='weighted')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#accuracy_scorer = make_scorer(accuracy_score)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39m# Grid Search\u001b[39;00m\n\u001b[0;32m     26\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, scoring\u001b[39m=\u001b[39mscoring, refit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train[feat5], y_train)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Get the best estimator and best hyperparameters\u001b[39;00m\n\u001b[0;32m     30\u001b[0m best_estimator \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:674\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m parameters\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    672\u001b[0m         cloned_parameters[k] \u001b[39m=\u001b[39m clone(v, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 674\u001b[0m     estimator \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcloned_parameters)\n\u001b[0;32m    676\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    678\u001b[0m X_train, y_train \u001b[39m=\u001b[39m _safe_split(estimator, X, y, train)\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:211\u001b[0m, in \u001b[0;36mPipeline.set_params\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_params\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    193\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Set the parameters of this estimator.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[39m    Valid parameter keys can be listed with ``get_params()``. Note that\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39m        Pipeline class instance.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_params(\u001b[39m\"\u001b[39;49m\u001b[39msteps\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    212\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\metaestimators.py:70\u001b[0m, in \u001b[0;36m_BaseComposition._set_params\u001b[1;34m(self, attr, **params)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replace_estimator(attr, name, params\u001b[39m.\u001b[39mpop(name))\n\u001b[0;32m     69\u001b[0m \u001b[39m# 3. Step parameters and other initialisation arguments\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:205\u001b[0m, in \u001b[0;36mBaseEstimator.set_params\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m valid_params:\n\u001b[0;32m    204\u001b[0m     local_valid_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_param_names()\n\u001b[1;32m--> 205\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid parameter \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m!r}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mValid parameters are: \u001b[39m\u001b[39m{\u001b[39;00mlocal_valid_params\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    210\u001b[0m \u001b[39mif\u001b[39;00m delim:\n\u001b[0;32m    211\u001b[0m     nested_params[key][sub_key] \u001b[39m=\u001b[39m value\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'activation' for estimator Pipeline(steps=[('StandardScaler', StandardScaler()),\n                ('undersampler', RandomUnderSampler(random_state=21)),\n                ('resampler', SMOTE(random_state=21)),\n                ('classifier', RandomForestClassifier(random_state=21))]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "pipe = Pipeline([('StandardScaler', StandardScaler()), ('pca', PCA()), (\"MLP\", MLPClassifier(random_state=21))])\n",
    "\n",
    "# Define hyperparameters for resampler and classifier\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100, 50), (100, 100)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'early_stopping': [True, False],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'tol': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "# Create custom scorers for F1 score and accuracy\n",
    "#f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "#accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Grid Search with multiple scoring metrics\n",
    "#scoring = {'F1': f1_scorer, 'Accuracy': accuracy_scorer}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search.fit(X_train[feat5], y_train)\n",
    "\n",
    "# Get the best estimator and best hyperparameters\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Access the results including all metrics\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Print the metrics for each combination of hyperparameters\n",
    "for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:\n",
    "    mean_metric_values = np.nanmean(results[f'mean_test_{metric}']) #results[f'mean_test_{metric}']\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_metric_values.mean():.4f}\")\n",
    "\n",
    "y_pred = best_estimator.predict(X_test[feat5])\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "precision = precision_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "roc_auc = roc_auc_score(y_test, y_pred)  # ROC AUC is for binary classification\n",
    "print(\"Best estimator metrics\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"ROC AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cenario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### TRAIN ####\n",
      "Accuracy: 0.98\n",
      "F1: 0.98\n",
      "Precision: 0.96\n",
      "Recall: 1.00\n",
      "ROC_AUC: 0.98\n",
      "#### TEST ####\n",
      "Accuracy: 0.72\n",
      "F1: 0.64\n",
      "Precision: 0.73\n",
      "Recall: 0.58\n",
      "ROC_AUC: 0.70\n"
     ]
    }
   ],
   "source": [
    "pipe = imblearnPipeline([('StandardScaler', StandardScaler()), ('under', RandomUnderSampler(random_state=21)), ('over', SMOTE(random_state=21)), (\"RandomForest\", RandomForestClassifier(random_state=21))])\n",
    "X_train_balanced, y_train_balanced = pipe['over'].fit_resample(X_train3[feat7], y_train3)\n",
    "pipe.fit(X_train_balanced, y_train_balanced)\n",
    "# Predict train dataset\n",
    "print(\"#### TRAIN ####\")\n",
    "y_pred = pipe.predict(X_train_balanced)\n",
    "print(\"Accuracy: %.2f\" % (accuracy_score(y_train_balanced, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_train_balanced, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_train_balanced, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_train_balanced, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_train_balanced, y_pred)))\n",
    "\n",
    "# Predict test dataset\n",
    "print(\"#### TEST ####\")\n",
    "y_pred = pipe.predict(X_test3[feat7])\n",
    "print(\"Accuracy: %.2f\" %(accuracy_score(y_test3, y_pred)))\n",
    "print(\"F1: %.2f\" %(f1_score(y_test3, y_pred)))\n",
    "print(\"Precision: %.2f\" %(precision_score(y_test3, y_pred)))\n",
    "print(\"Recall: %.2f\" %(recall_score(y_test3, y_pred)))\n",
    "print(\"ROC_AUC: %.2f\" %(roc_auc_score(y_test3, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "945 fits failed out of a total of 1215.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "405 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 550, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 409, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "540 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 208, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\base.py\", line 108, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "                              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 550, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\imblearn\\utils\\_validation.py\", line 389, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to remove samples from the minority class while trying to generate new samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.71342801        nan 0.69024227        nan        nan        nan\n",
      "        nan        nan        nan 0.70817043        nan 0.70711779\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71131718        nan 0.71554999        nan        nan        nan\n",
      "        nan        nan        nan 0.68288499        nan 0.71761626\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71233083        nan 0.71870231        nan        nan        nan\n",
      "        nan        nan        nan 0.70708995        nan 0.71447508\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66598162        nan 0.69239209        nan        nan        nan\n",
      "        nan        nan        nan 0.71340017        nan 0.70815372\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71234197        nan 0.71660819        nan        nan        nan\n",
      "        nan        nan        nan 0.66916736        nan 0.68290727\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71657477        nan 0.70818713        nan        nan        nan\n",
      "        nan        nan        nan 0.71131161        nan 0.71765525\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68815372        nan 0.69549986        nan        nan        nan\n",
      "        nan        nan        nan 0.71658034        nan 0.71448622\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70815929        nan 0.72186021        nan        nan        nan\n",
      "        nan        nan        nan 0.68496798        nan 0.69337789\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.72290727        nan 0.70921192        nan        nan        nan\n",
      "        nan        nan        nan 0.72395433        nan 0.71975494\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71342801        nan 0.68918964        nan        nan        nan\n",
      "        nan        nan        nan 0.71343358        nan 0.71238652\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71131718        nan 0.71133389        nan        nan        nan\n",
      "        nan        nan        nan 0.68288499        nan 0.71340574\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71549986        nan 0.71342801        nan        nan        nan\n",
      "        nan        nan        nan 0.70813701        nan 0.71236981\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66598162        nan 0.69344472        nan        nan        nan\n",
      "        nan        nan        nan 0.71023113        nan 0.71131718\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71234754        nan 0.71766082        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.62825096        nan 0.5853477         nan        nan        nan\n",
      "        nan        nan        nan 0.63621038        nan 0.62434339\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64206123        nan 0.63501991        nan        nan        nan\n",
      "        nan        nan        nan 0.61985914        nan 0.63670746\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64430672        nan 0.64114985        nan        nan        nan\n",
      "        nan        nan        nan 0.63596706        nan 0.63348149\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59976624        nan 0.61230869        nan        nan        nan\n",
      "        nan        nan        nan 0.64682734        nan 0.62601778\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64489915        nan 0.63841876        nan        nan        nan\n",
      "        nan        nan        nan 0.59649502        nan 0.5946805\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64487846        nan 0.62365559        nan        nan        nan\n",
      "        nan        nan        nan 0.6417365         nan 0.63719436\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62725149        nan 0.6174117         nan        nan        nan\n",
      "        nan        nan        nan 0.6427445         nan 0.63926486\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63145513        nan 0.64190254        nan        nan        nan\n",
      "        nan        nan        nan 0.61911362        nan 0.61606421\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6542432         nan 0.63308794        nan        nan        nan\n",
      "        nan        nan        nan 0.65701329        nan 0.64377152\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62948011        nan 0.585727          nan        nan        nan\n",
      "        nan        nan        nan 0.63949977        nan 0.63148068\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64219           nan 0.63256483        nan        nan        nan\n",
      "        nan        nan        nan 0.61985914        nan 0.6326254\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64597483        nan 0.63622499        nan        nan        nan\n",
      "        nan        nan        nan 0.63876328        nan 0.63181144\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59976624        nan 0.61347497        nan        nan        nan\n",
      "        nan        nan        nan 0.64417207        nan 0.62974354\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64678203        nan 0.63988754        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.63993455        nan 0.61608758        nan        nan        nan\n",
      "        nan        nan        nan 0.6218057         nan 0.62913067\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62364099        nan 0.64202794        nan        nan        nan\n",
      "        nan        nan        nan 0.5888992         nan 0.64412756\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62455727        nan 0.64044109        nan        nan        nan\n",
      "        nan        nan        nan 0.61968491        nan 0.63832192\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56533038        nan 0.60770089        nan        nan        nan\n",
      "        nan        nan        nan 0.62426081        nan 0.63084154\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62467699        nan 0.63877658        nan        nan        nan\n",
      "        nan        nan        nan 0.57343701        nan 0.5993632\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63217561        nan 0.63127815        nan        nan        nan\n",
      "        nan        nan        nan 0.62474693        nan 0.64345528\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.59166006        nan 0.60906318        nan        nan        nan\n",
      "        nan        nan        nan 0.63502333        nan 0.63569834\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62499278        nan 0.64966263        nan        nan        nan\n",
      "        nan        nan        nan 0.58818699        nan 0.60437368\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64064873        nan 0.62699596        nan        nan        nan\n",
      "        nan        nan        nan 0.64073317        nan 0.64306059\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63896681        nan 0.61411549        nan        nan        nan\n",
      "        nan        nan        nan 0.63148031        nan 0.63497408\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62386656        nan 0.63437351        nan        nan        nan\n",
      "        nan        nan        nan 0.5888992         nan 0.63771405\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6304892         nan 0.63299321        nan        nan        nan\n",
      "        nan        nan        nan 0.62017439        nan 0.63479073\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.56533038        nan 0.60975741        nan        nan        nan\n",
      "        nan        nan        nan 0.61961469        nan 0.63567649\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62331564        nan 0.64058424        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.62108108        nan 0.55935135        nan        nan        nan\n",
      "        nan        nan        nan 0.65347748        nan 0.62381982\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66403604        nan 0.63192793        nan        nan        nan\n",
      "        nan        nan        nan 0.65877477        nan 0.63196396\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66663063        nan 0.64551351        nan        nan        nan\n",
      "        nan        nan        nan 0.656             nan 0.63192793\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63974775        nan 0.62100901        nan        nan        nan\n",
      "        nan        nan        nan 0.67473874        nan 0.62655856\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66944144        nan 0.64277477        nan        nan        nan\n",
      "        nan        nan        nan 0.62641441        nan 0.59434234\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66140541        nan 0.62118919        nan        nan        nan\n",
      "        nan        nan        nan 0.66407207        nan 0.63455856\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66947748        nan 0.62897297        nan        nan        nan\n",
      "        nan        nan        nan 0.65333333        nan 0.64807207\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.64263063        nan 0.64003604        nan        nan        nan\n",
      "        nan        nan        nan 0.656             nan 0.63466667\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67218018        nan 0.64263063        nan        nan        nan\n",
      "        nan        nan        nan 0.67754955        nan 0.64803604\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.62378378        nan 0.56201802        nan        nan        nan\n",
      "        nan        nan        nan 0.65073874        nan 0.63192793\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66407207        nan 0.63459459        nan        nan        nan\n",
      "        nan        nan        nan 0.65877477        nan 0.62922523\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66396396        nan 0.64281081        nan        nan        nan\n",
      "        nan        nan        nan 0.66133333        nan 0.63192793\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.63974775        nan 0.62100901        nan        nan        nan\n",
      "        nan        nan        nan 0.67477477        nan 0.62926126\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.67481081        nan 0.64544144        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\afons\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.69703979        nan 0.66705199        nan        nan        nan\n",
      "        nan        nan        nan 0.69848537        nan 0.69235969\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70292506        nan 0.70076157        nan        nan        nan\n",
      "        nan        nan        nan 0.67865275        nan 0.70251122\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70422236        nan 0.70582272        nan        nan        nan\n",
      "        nan        nan        nan 0.69804498        nan 0.699892\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66129816        nan 0.67970241        nan        nan        nan\n",
      "        nan        nan        nan 0.7065148         nan 0.69375154\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70475071        nan 0.70357629        nan        nan        nan\n",
      "        nan        nan        nan 0.661618          nan 0.66729111\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70680465        nan 0.69282848        nan        nan        nan\n",
      "        nan        nan        nan 0.70292809        nan 0.70294645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.68487367        nan 0.68371437        nan        nan        nan\n",
      "        nan        nan        nan 0.70534733        nan 0.70280665\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.6965477         nan 0.70742431        nan        nan        nan\n",
      "        nan        nan        nan 0.67981409        nan 0.68304548\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71390868        nan 0.69745475        nan        nan        nan\n",
      "        nan        nan        nan 0.71574629        nan 0.70709898\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69752907        nan 0.66666118        nan        nan        nan\n",
      "        nan        nan        nan 0.70231839        nan 0.69814537\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70293559        nan 0.69776206        nan        nan        nan\n",
      "        nan        nan        nan 0.67865275        nan 0.69853315\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70637479        nan 0.70100061        nan        nan        nan\n",
      "        nan        nan        nan 0.69984208        nan 0.69815287\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.66129816        nan 0.68056447        nan        nan        nan\n",
      "        nan        nan        nan 0.70392412        nan 0.69684202\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.70569626        nan 0.70491712        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'classifier__max_depth': 10, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 100, 'resampler__sampling_strategy': 'auto', 'undersampler__sampling_strategy': 'auto'}\n",
      "Mean Accuracy: 0.7059\n",
      "Mean F1_score: 0.6302\n",
      "Mean Precision: 0.6228\n",
      "Mean Recall: 0.6421\n",
      "Mean Roc_auc: 0.6946\n",
      "Best estimator metrics\n",
      "Accuracy: 0.7248157248157249\n",
      "F1 Score: 0.7238498686774549\n",
      "Precision: 0.7237686478192807\n",
      "Recall: 0.7248157248157249\n",
      "ROC AUC: 0.718220753793441\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score, accuracy_score\n",
    "\n",
    "pipeline = imblearnPipeline([('StandardScaler', StandardScaler()), ('undersampler', RandomUnderSampler(random_state=21)), ('resampler', SMOTE(random_state=21)), (\"classifier\", RandomForestClassifier(random_state=21))])\n",
    "\n",
    "# Define hyperparameters for resampler and classifier\n",
    "param_grid = {\n",
    "    'resampler__sampling_strategy': ['auto', 0.5, 0.75],\n",
    "    'undersampler__sampling_strategy': ['auto', 0.5, 0.75],\n",
    "    'classifier__n_estimators': [10, 50, 100],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create custom scorers for F1 score and accuracy\n",
    "#f1_scorer = make_scorer(f1_score, average='weighted')\n",
    "#accuracy_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Grid Search with multiple scoring metrics\n",
    "#scoring = {'F1': f1_scorer, 'Accuracy': accuracy_scorer}\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring=scoring, refit='f1_score')\n",
    "grid_search.fit(X_train3[feat7], y_train3)\n",
    "\n",
    "# Get the best estimator and best hyperparameters\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Access the results including all metrics\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Print the metrics for each combination of hyperparameters\n",
    "for metric in ['accuracy', 'f1_score', 'precision', 'recall', 'roc_auc']:\n",
    "    mean_metric_values = np.nanmean(results[f'mean_test_{metric}']) #results[f'mean_test_{metric}']\n",
    "    print(f\"Mean {metric.capitalize()}: {mean_metric_values.mean():.4f}\")\n",
    "\n",
    "y_pred = best_estimator.predict(X_test3[feat7])\n",
    "accuracy = accuracy_score(y_test3, y_pred)\n",
    "f1 = f1_score(y_test3, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "precision = precision_score(y_test3, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "recall = recall_score(y_test3, y_pred, average='weighted')  # Adjust 'average' as needed\n",
    "roc_auc = roc_auc_score(y_test3, y_pred)  # ROC AUC is for binary classification\n",
    "print(\"Best estimator metrics\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"ROC AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
